{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте поприпарируем немного линейную регрессию:\n",
    "\n",
    "Пусть задано множество ${y_i}$  $i=\\overline{1..n}$\n",
    "\n",
    "1) Решим задачу регрессии вида $y = \\beta + \\epsilon$\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = \\beta \n",
    "$$\n",
    "Решим с помощью МНК:\n",
    "$$\n",
    "min(\\sum_{i=1}^{N}\\left(\\ y_i - \\hat{y_i})\\ \\right) ^ 2) = min(\\sum_{i=1}^{N}\\left(\\ y_i - \\beta)\\ \\right) ^ 2 \\\\\n",
    "-2\\sum_{i=1}^{N}(y_i - \\beta) = 0 \\\\\n",
    "\\beta = \\frac{\\sum_{i=1}^{N}(y_i)}{N} \\\\\n",
    "$$\n",
    "\n",
    "Теперь давайте посмотрим на геометрическую интерпретацию полученного решения. Пусть нам дан вектор $y=(y_1, y_2, ... y_n)$, обозначим за $\\bar{1}$ вектор вида $(1, 1, ...)$, и за $\\hat{y} = \\bar{y} \\bar{1}$ вектор наших оценок, полученных простой моделью регрессии на константу. Очевидно, из геометрической интерпретации, что $y=\\hat{y} + \\bar{\\epsilon_i}$. \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N}\\hat{y_i}=\\sum_{i=1}^{N}\\bar{y}=n*\\bar{y}=\\sum_{i=1}^{N}y_i \\\\\n",
    "\\sum_{i=1}^{N}\\hat{y_i} = \\sum_{i=1}^{N}y_i \\\\\n",
    "\\sum_{i=1}^{N}(\\hat{y_i} - y_i) = 0 \\\\\n",
    "\\sum_{i=1}^{N}\\epsilon_i = 0 \\\\\n",
    "\\sum_{i=1}^{N}\\epsilon_i * 1 = 0 \\\\\n",
    "(\\bar{\\epsilon}, \\bar{1}) = 0 \\\\\n",
    "\\bar{\\epsilon} \\perp \\bar{1}\n",
    "$$\n",
    "\n",
    "Что это значит? Это значит, что проекция вектора $y=(y_1, y_2, ... y_n)$ на вектор из единиц получим оценку вектора $\\bar{\\hat{y}}$ в модели с константным предиктором. Кажется просто? И какой из этого толк?\n",
    "\n",
    "Это просто подготовка: усложним задачу.\n",
    "\n",
    "2) Решим задачу регрессии вида $y = \\beta_0 + \\beta_1 * x + \\beta_2 * z$\n",
    "\n",
    "Какие выводы можем сделать из предыдущего шага?\n",
    "$$\n",
    "\\bar{\\epsilon} \\perp \\bar{1} \\\\\n",
    "\\bar{x} \\perp \\bar{\\epsilon} \\\\\n",
    "\\bar{\\epsilon} \\perp \\bar{z}\n",
    "$$\n",
    "Отсюда можно получить оценки на все коэффициенты, но важно не это. Линейная комбинация векторов перпендикулярных заданному - перпендикулярна ему. То есть $\\hat{y} \\perp \\hat{\\epsilon}$, значит у нас по-прежнему прямоугольный треугольник. Но заметим еще, что из условий 1 порядка $\\sum\\epsilon_i = 0$, откуда $\\sum\\hat{y_i}=\\sum(y_i)$ и $\\bar{y}=\\bar{\\hat{y}}$. То есть проекции векторов предсказаний и целевых переменных попадут в 1 точку на векторе $\\bar{1}$. По теореме о 3-х перпендикулярах получаем прямоугольный треугольник. Давайте порисуем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три разные суммы квадратов:\n",
    "\n",
    "- RSS - сумма квадратов остатков\n",
    "- TSS - общая сумма квадратов\n",
    "- ESS - объясненная сумма квадратов\n",
    "$$\n",
    "RSS = \\sum(\\epsilon_i^2) \\\\\n",
    "TSS = \\sum(y_i-\\bar{y})^2 \\\\\n",
    "ESS = \\sum(\\hat{y_i}-\\bar{y})^2\n",
    "$$\n",
    "\n",
    "На основании рисунка выше мы доказали, что TSS = RSS + ESS. При этом \n",
    "$$\n",
    "\\frac{ESS}{TSS} = R^2 = sCorr(\\hat{y}, y)\n",
    "$$\n",
    "Это доля объясненного разброса Y в общем разбросе y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как узнать, есть ли пропущенные переменные в модели? тест Рамсея."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Деревья\n",
    "\n",
    "Краткая идея: разобьем пространство признаком на регионы. В каждом регионе будет предсказывать среднее. Строится дерево в прямом смысле.\n",
    "\n",
    "<img src=\"trees_common_view.png\"/>\n",
    "\n",
    "\n",
    "Какие проблемы?\n",
    "- как разбить?\n",
    "- что, если приходят данные далекие от полученных разбиений?\n",
    "- как посмотреть?\n",
    "\n",
    "1) Как разбить?\n",
    "CART - строим все возможные (в дискретном смысле) гиперплоскости, которые разбивали бы наше пространство на два. Выбираем разбиение минимизирующее метрику. На следующих итерациях мы берем один худший лист и проводим ту же операцию по разбиению его. Продолжаем так делать, пока не достигнем ограничения по количеству узлов, либо от одной итерации к другой перестанет улучшаться общая ошибка  - переобучаемся. Что делать? pruning, ограничение на число элементов в листе.\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i \\in S_1}(y_i−\\bar{y_1})+\\sum_{i \\in S_2}(y_i−\\bar{y_2}) + \\gamma * treeSize\n",
    "$$\n",
    "\n",
    "2) Что, если приходят совсем новые данные?\n",
    "\n",
    "Интерполяции не происходит. Предсказание последнего разбиения.\n",
    "\n",
    "<img src=\"trees_predictions.png\"/>\n",
    "\n",
    "Попробуем сделать не дерево, но пень!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(threshold, x, y):\n",
    "    #TODO реализовать функцию подсчета лосса\n",
    "\n",
    "def decision_stump(x, y):\n",
    "    #TODO реализовать решающий пень, подобрать лучший порог\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В строенной библиотеке sklearn уже все сделано за нас и для нас. Давайте сгенерируем выборку и попробуем обучить дерево. Также посмотрим, что нам предсказывает решающий лес."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "n_train = 150        \n",
    "n_test = 1000       \n",
    "noise = 0.1\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2) + \\\n",
    "        np.random.normal(0.0, noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise)\n",
    "\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "d_predict = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, d_predict, \"g\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Decision tree regressor, MSE = %.2f\" % np.sum((y_test - d_predict) ** 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В питон встроены приспособления для визуализации деревьев. Код ниже может не заработать - требует установки graphiz. Давайте попробуем отрисовать полученное решающее дерево."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, feature_names=['f1'], out_file='example.dot')\n",
    "!dot -Tpng 'example.dot' -o 'example.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names):\n",
    "        left      = tree.tree_.children_left\n",
    "        right     = tree.tree_.children_right\n",
    "        threshold = tree.tree_.threshold\n",
    "        features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "        value = tree.tree_.value\n",
    "\n",
    "        def recurse(left, right, threshold, features, node):\n",
    "                if (threshold[node] != -2):\n",
    "                        print(\"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\")\n",
    "                        if left[node] != -1:\n",
    "                                recurse (left, right, threshold, features,left[node])\n",
    "                        print(\"} else {\")\n",
    "                        if right[node] != -1:\n",
    "                                recurse (left, right, threshold, features,right[node])\n",
    "                        print(\"}\")\n",
    "                else:\n",
    "                        print(\"return \" + str(value[node]))\n",
    "\n",
    "        recurse(left, right, threshold, features, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rules(tree, feature_names=['f1', 'f2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сравним предсказания деревьев и линейной регрессии на сгенерированной выборке."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
